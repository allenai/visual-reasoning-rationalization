text_only = {
    "train_data_file": "/data/vqa/annotations/VQA-E_train_set.json",
    "output_dir": "/models/vqa/q_a_to_r/",
    "task": "vqa",
    "model_type": "gpt2_vcr",
    "model_name_or_path": "gpt2",
    "per_gpu_train_batch_size": 32,
    "per_gpu_eval_batch_size": 32,
    "eval_data_file": "/data/vqa/annotations/VQA-E_val_set.json",
    "save_steps": 3000,
    "num_train_epochs": 5,
    "do_train": True,
    "learning_rate": 5e-5,
    "logging_steps": 100,
    "do_eval": True,
    "text_only": True,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": -1,
    "do_padding": False,
    "max_question_length": 19, # 73
    "max_answer_length": 23, # 61
    "max_rationale_length": 51, # 175
    "block_size": 93, # 309
    "max_situation_length": -1,
    "max_viscomet_inferences_length": -1,
    "use_token_type_ids": True
}

text_only_eval = {
    "model_type": "gpt2_vcr",
    "task": "vqa",
    "eval_data_file":  "/data/vqa/annotations/VQA-E_val_set_random250.json",
    "no_cache": True,
    "text_only": True,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": -1,
    "top_k": 1,
    "top_p": 0.0,
    "temperature": 1.0,
    "overwrite_cache": True,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "block_size": 93,
    "length": 50,
    "max_situation_length": -1,
    "max_viscomet_inferences_length": -1,
    "use_token_type_ids": True
}

textual_objects = {
    "train_data_file": "/data/vqa/annotations/VQA-E_train_set.json",
    "output_dir": "/models/vqa/text_objects_q_a_to_r/",
    "task": "vqa",
    "model_type": "gpt2_vcr",
    "model_name_or_path": "gpt2",
    "per_gpu_train_batch_size": 32,
    "per_gpu_eval_batch_size": 32,
    "eval_data_file": "/data/vqa/annotations/VQA-E_val_set.json",
    "save_steps": 3000,
    "num_train_epochs": 5,
    "do_train": True,
    "learning_rate": 5e-5,
    "logging_steps": 100,
    "do_eval": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": True,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": 30,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "max_viscomet_inferences_length": -1,
    "block_size": 123,
    "max_situation_length": -1,
    "use_token_type_ids": True
}

textual_objects_eval = {
    "model_type": "gpt2_vcr",
    "task": "vqa",
    "eval_data_file": "/data/vqa/annotations/VQA-E_val_set_random250.json",
    "no_cache": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": True,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": 30,
    "top_k": 1,
    "top_p": 0,
    "temperature": 1.0,
    "overwrite_cache": True,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "max_viscomet_inferences_length": -1,
    "block_size": 123,
    "length": 50,
    "max_situation_length": -1,
    "use_token_type_ids": True
}

embedding_objects = {
    "train_data_file": "/data/vqa/annotations/VQA-E_train_set.json",
    "output_dir": "/models/vqa/embed_objects_q_a_to_r/",
    "task": "vqa",
    "model_type": "gpt2_vcr",
    "model_name_or_path": "gpt2",
    "per_gpu_train_batch_size": 32,
    "per_gpu_eval_batch_size": 32,
    "eval_data_file": "/data/vqa/annotations/VQA-E_val_set.json",
    "save_steps": 3000,
    "num_train_epochs": 5,
    "do_train": True,
    "learning_rate": 5e-5,
    "logging_steps": 100,
    "do_eval": True,
    "text_only": False,
    "embeddings_object_detector": True,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": 28,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "block_size": 123,
    "max_situation_length": -1,
    "use_token_type_ids": True,
    "vision_embd": 2048,
    "n_coordinates": 5,
    "max_viscomet_inferences_length": -1
}

embedding_objects_eval = {
    "model_type": "gpt2_vcr",
    "task": "vqa",
    "eval_data_file": "/data/vqa/annotations/VQA-E_val_set_random250.json",
    "no_cache": True,
    "text_only": False,
    "embeddings_object_detector": True,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": 28,
    "top_k": 1,
    "top_p": 0.0,
    "temperature": 1.0,
    "overwrite_cache": True,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "block_size": 123,
    "length": 50,
    "max_situation_length": -1,
    "use_token_type_ids": True,
    "max_viscomet_inferences_length": -1
}

textual_situation = {
    "train_data_file": "/data/vqa/annotations/VQA-E_train_set_situation.json",
    "output_dir": "/models/vqa/text_situation_q_a_to_r/",
    "task": "vqa",
    "model_type": "gpt2_vcr",
    "model_name_or_path": "gpt2",
    "per_gpu_train_batch_size": 32,
    "per_gpu_eval_batch_size": 32,
    "eval_data_file": "/data/vqa/annotations/VQA-E_val_set_situation.json",
    "save_steps": 3000,
    "num_train_epochs": 5,
    "do_train": True,
    "learning_rate": 5e-5,
    "logging_steps": 100,
    "do_eval": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": True,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": -1,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "max_situation_length": 17,
    "block_size": 112,
    "max_viscomet_inferences_length": -1,
    "use_token_type_ids": True
}

textual_situation_eval = {
    "model_type": "gpt2_vcr",
    "task": "vqa",
    "eval_data_file": "/data/vqa/annotations/VQA-E_val_set_random250_situation.json",
    "no_cache": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": True,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": -1,
    "top_k": 1,
    "top_p": 0,
    "temperature": 1.0,
    "overwrite_cache": True,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "max_situation_length": 17,
    "max_viscomet_inferences_length": -1,
    "block_size": 112,
    "length": 50,
    "use_token_type_ids": True
}

embedding_situation = {
    "train_data_file": "/data/vqa/annotations/VQA-E_train_set.json",
    "output_dir": "/models/vqa/embed_situ_q_a_to_r/",
    "task": "vqa",
    "model_type": "gpt2_vcr",
    "model_name_or_path": "gpt2",
    "per_gpu_train_batch_size": 32,
    "per_gpu_eval_batch_size": 32,
    "eval_data_file": "/data/vqa/annotations/VQA-E_val_set.json",
    "save_steps": 3000,
    "num_train_epochs": 5,
    "do_train": True,
    "learning_rate": 5e-5,
    "logging_steps": 100,
    "do_eval": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": True,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": 7,
    "max_object_num": -1,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "block_size": 102,
    "max_situation_length": -1,
    "max_viscomet_inferences_length": -1,
    "use_token_type_ids": True,
    "vision_embd": 2048,
    "n_coordinates": 5
}

embedding_situation_eval = {
    "model_type": "gpt2_vcr",
    "task": "vqa",
    "eval_data_file": "/data/vqa/annotations/VQA-E_val_set.json",
    "no_cache": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": True,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": False,
    "max_situ_role_num": 6,
    "max_object_num": -1,
    "top_k": 1,
    "top_p": 0,
    "temperature": 1.0,
    "overwrite_cache": True,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "block_size": 102,
    "max_viscomet_inferences_length": -1,
    "length": 50,
    "max_situation_length": -1,
    "use_token_type_ids": True
}

textual_viscomet = {
    "train_data_file": "/data/vqa/annotations/vqa_train_viscomet_inferences.json",
    "output_dir": "/models/vqa/textual_viscomet_q_a_to_r/",
    "task": "vqa",
    "model_type": "gpt2_vcr",
    "model_name_or_path": "gpt2",
    "per_gpu_train_batch_size": 32,
    "per_gpu_eval_batch_size": 32,
    "eval_data_file": "/data/vqa/annotations/vqa_val_viscomet_inferences.json",
    "save_steps": 3000,
    "num_train_epochs": 5,
    "do_train": True,
    "learning_rate": 5e-5,
    "logging_steps": 100,
    "do_eval": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": True,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": -1,
    "do_padding": False,
    "max_question_length": 19, # 73
    "max_answer_length": 23, # 61
    "max_rationale_length": 51, # 175
    "block_size": 241, # 309
    "max_situation_length": -1,
    "max_viscomet_inferences_length": 148,
    "use_token_type_ids": True
}

textual_viscomet_eval = {
    "model_type": "gpt2_vcr",
    "task": "vqa",
    "eval_data_file":  "/data/vqa/annotations/vqa_val_viscomet_inferences_random250.json",
    "no_cache": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": True,
    "embeddings_viscomet": False,
    "max_situ_role_num": -1,
    "max_object_num": -1,
    "top_k": 1,
    "top_p": 0.0,
    "temperature": 1.0,
    "overwrite_cache": True,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "block_size": 241,
    "length": 50,
    "max_situation_length": -1,
    "max_viscomet_inferences_length": 148,
    "use_token_type_ids": True
}


embeddings_viscomet = {
    "train_data_file": "/data/vqa/annotations/vqa_train_viscomet_inferences.json",
    "output_dir": "/models/vqa/embeddings_viscomet_q_a_to_r/",
    "task": "vqa",
    "model_type": "gpt2_vcr",
    "model_name_or_path": "gpt2",
    "per_gpu_train_batch_size": 32,
    "per_gpu_eval_batch_size": 32,
    "eval_data_file": "/data/vqa/annotations/vqa_val_viscomet_inferences.json",
    "save_steps": 3000,
    "num_train_epochs": 5,
    "do_train": True,
    "learning_rate": 5e-5,
    "logging_steps": 100,
    "do_eval": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": True,
    "max_situ_role_num": -1,
    "max_object_num": -1,
    "do_padding": False,
    "max_question_length": 19, # 73
    "max_answer_length": 23, # 61
    "max_rationale_length": 51, # 175
    "block_size": 98, # 309
    "max_situation_length": -1,
    "max_viscomet_inferences_length": -1,
    "use_token_type_ids": True
}

embeddings_viscomet_eval = {
    "model_type": "gpt2_vcr",
    "task": "vqa",
    "eval_data_file": "/data/vqa/annotations/vqa_val_viscomet_inferences_random250.json",
    "no_cache": True,
    "text_only": False,
    "embeddings_object_detector": False,
    "embeddings_situation_recognizer": False,
    "textual_output_object_detector": False,
    "textual_output_situation_recognizer": False,
    "textual_viscomet_inferences": False,
    "embeddings_viscomet": True,
    "max_situ_role_num": -1,
    "max_object_num": -1,
    "top_k": 1,
    "top_p": 0.0,
    "temperature": 1.0,
    "overwrite_cache": True,
    "do_padding": False,
    "max_question_length": 19,
    "max_answer_length": 23,
    "max_rationale_length": 51,
    "block_size": 98,
    "length": 50,
    "max_situation_length": -1,
    "max_viscomet_inferences_length": -1,
    "use_token_type_ids": True
}
HYPERPARAMETERS = {
    "text_only": text_only,
    "text_only_eval": text_only_eval,
    "textual_objects": textual_objects,
    "textual_objects_eval": textual_objects_eval,
    "embedding_objects": embedding_objects,
    "embedding_objects_eval": embedding_objects_eval,
    "textual_situation": textual_situation,
    "textual_situation_eval": textual_situation_eval,
    "embedding_situation": embedding_situation,
    "embedding_situation_eval": embedding_situation_eval,
    "textual_viscomet": textual_viscomet,
    "textual_viscomet_eval": textual_viscomet_eval,
    "embeddings_viscomet": embeddings_viscomet,
    "embeddings_viscomet_eval": embeddings_viscomet_eval
}

